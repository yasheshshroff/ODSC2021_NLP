{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/yashroff/miniconda3/envs/pytorch/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: click in /home/yashroff/miniconda3/envs/pytorch/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /home/yashroff/miniconda3/envs/pytorch/lib/python3.8/site-packages (from nltk) (4.58.0)\n",
      "Requirement already satisfied: regex in /home/yashroff/miniconda3/envs/pytorch/lib/python3.8/site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: joblib in /home/yashroff/miniconda3/envs/pytorch/lib/python3.8/site-packages (from nltk) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00001-80603ce7-5e43-4aaa-9c2e-fc333c20a663",
    "executionInfo": {
     "elapsed": 1675,
     "status": "ok",
     "timestamp": 1603916886579,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 4,
    "execution_start": 1603914492064,
    "id": "09g62Azr5U1R",
    "output_cleared": false,
    "source_hash": "bbacf3a0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stemming\n",
    "\n",
    "* Base words for stemming & stop words\n",
    "* Helps reduce the amount of training data that we would need to train our models by capturing different variations of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# Alt\n",
    "# nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer1 = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish\n",
      "fish\n",
      "fish\n"
     ]
    }
   ],
   "source": [
    "# The stem for fishes, fishing, fished is \"fish\"\n",
    "print(stemmer1.stem(\"fishing\"))\n",
    "print(stemmer1.stem(\"fishes\"))\n",
    "print(stemmer1.stem(\"fished\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer2 = SnowballStemmer(\"english\", ignore_stopwords=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have\n",
      "having\n"
     ]
    }
   ],
   "source": [
    "print(stemmer1.stem(\"having\")) # no stop words\n",
    "print(stemmer2.stem(\"having\")) # stop words not stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fishing'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"fishing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"corpora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Hi, how are you doing? I am fine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = s.lower()\n",
    "t1 = t1.replace(\",\", \"\")\n",
    "t1 = t1.replace(\"?\", \"\")\n",
    "t1 = t1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'how', 'are', 'you', 'doing']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'how', 'are', 'you', 'doing', 'I', 'am', 'fine']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the tokens for building ML model\n",
    "s2 = \"Hi, how are you doing? I am fine\"\n",
    "re.sub(r\"[^\\w]\", \" \", s2).split() # replace punctuations with a space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. NLTK based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', ',', 'how', 'are', 'you', 'doing', '?', 'I', 'am', 'fine']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize # regex tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi, how are you doing?', 'I am fine']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Bag of Words\n",
    "\n",
    "Bag of Words based encoding or TF-IDF vector is a frequentist based approach to NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00002-c68972ca-99d0-4fb8-8b2c-1c296e224a4a",
    "executionInfo": {
     "elapsed": 849,
     "status": "ok",
     "timestamp": 1603916888361,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 15,
    "execution_start": 1603914493054,
    "id": "kOWus-a95U1Z",
    "output_cleared": false,
    "source_hash": "2f335b27",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/2554/2554.txt\"\n",
    "\n",
    "response = requests.get(url)\n",
    "raw_html = response.content\n",
    "text = raw_html.decode(\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00006-381941ee-0649-4a76-9841-1923426edffe",
    "executionInfo": {
     "elapsed": 2459,
     "status": "ok",
     "timestamp": 1603917274958,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 248,
    "execution_start": 1603914496124,
    "id": "iSDR5KNt5U1g",
    "output_cleared": false,
    "source_hash": "abe7455d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/829/829-0.txt\" # gulliver's travels\n",
    "#Alt\n",
    "#url = 'https://www.gutenberg.org/files/2701/2701-0.txt' # Moby Dick\n",
    "\n",
    "file = urllib.request.urlopen(url)\n",
    "text = [line.decode('utf-8') for line in file]\n",
    "text = ''.join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-e8474323-77fb-4af2-b76b-cf21b17e2b07",
    "id": "S3hK5Ldw5U1j",
    "tags": []
   },
   "source": [
    "### 6. Tokenizing with punkt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00008-c92e3a1e-acf6-4466-8344-0ca3510a6b75",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1306,
     "status": "ok",
     "timestamp": 1603917279960,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 2119,
    "execution_start": 1603914497851,
    "id": "43g6RdUf5U1k",
    "outputId": "243880a7-7b85-4f86-98b9-379827ce3afb",
    "output_cleared": false,
    "source_hash": "dd04af38",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yashroff/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00009-e513d93c-17df-4f64-ba5e-748f1628283d",
    "executionInfo": {
     "elapsed": 647,
     "status": "ok",
     "timestamp": 1603917288906,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 218,
    "execution_start": 1603914500757,
    "id": "oc9TB80i5U1p",
    "output_cleared": false,
    "source_hash": "204a6f56",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "tokens = [w.translate(table) for w in tokens]\n",
    "tokens = [word.lower() for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-128bff7f-6c9c-41c7-af8b-34e14d7c8cda",
    "id": "d-kUPb7k5U1s",
    "tags": []
   },
   "source": [
    "Removing **stop-words** and **stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00011-cdf4882f-23c8-4056-852c-7aa7f2e398d9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1571,
     "status": "ok",
     "timestamp": 1603917291637,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 2889,
    "execution_start": 1603914502637,
    "id": "Syfo7BOb5U1t",
    "outputId": "842060a1-2a34-4b07-c1d2-e90caa8c4d71",
    "output_cleared": false,
    "source_hash": "203b141e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yashroff/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['travel', 'littl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "tokens = [porter.stem(word) for word in tokens]\n",
    "tokens[200:202]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-044ce2ce-6815-490b-9f18-0645470e543f",
    "id": "NjzIKn_85U1w",
    "tags": []
   },
   "source": [
    "**Understanding the vocabulary** \n",
    "\n",
    "* A vocabulary of a document represents all the words in that document and the frequency they appear.\n",
    "* `FreqDist` class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00013-fdd3703a-21ab-4f5c-bef8-ecccb46953e3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1603917300119,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 40,
    "execution_start": 1603914505591,
    "id": "2Yv0397E5U1x",
    "outputId": "9ceba789-32ca-425d-af6c-26b40c38e744",
    "output_cleared": false,
    "source_hash": "60a04f4f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'could': 395, 'upon': 393, 'would': 370, 'great': 298, 'one': 288, 'two': 252, 'time': 240, 'countri': 232, 'made': 228, 'much': 212, ...})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "word_counts = FreqDist(tokens)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-72d4693b-04ed-488c-ae66-8f0ce7199aed",
    "id": "RhDlzRqE5U1z",
    "tags": []
   },
   "source": [
    "**Scoring words with frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00015-02387729-4804-4ce6-9161-419c503be606",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1603917302419,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 9,
    "execution_start": 1603914506105,
    "id": "eapZUCqh5U10",
    "outputId": "54d8f9b6-ffa1-4158-dcd9-aa00c84730f5",
    "output_cleared": false,
    "source_hash": "d999d5d7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('could', 395),\n",
       " ('upon', 393),\n",
       " ('would', 370),\n",
       " ('great', 298),\n",
       " ('one', 288),\n",
       " ('two', 252),\n",
       " ('time', 240),\n",
       " ('countri', 232),\n",
       " ('made', 228),\n",
       " ('much', 212)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = 100\n",
    "vocabulary = word_counts.most_common(top)\n",
    "\n",
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "00016-a40281d3-efa8-460f-bbe0-e4090eee870a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 599,
     "status": "ok",
     "timestamp": 1603917303987,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 4,
    "execution_start": 1603914507044,
    "id": "SO1eID8a5U14",
    "outputId": "70e04656-fbc9-4c47-a1f0-36bf5282b984",
    "output_cleared": false,
    "source_hash": "a2651e8b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 191)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size = len(vocabulary)\n",
    "doc_vector = np.zeros(voc_size)\n",
    "\n",
    "word_vector = [(idx,word_counts[word[0]]) for idx, word in enumerate(vocabulary) if word[0] in word_counts.keys()] \n",
    "word_vector[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "00017-36e9c930-b190-4fa9-9b2d-dbda227ca607",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3213,
     "status": "ok",
     "timestamp": 1603917335032,
     "user": {
      "displayName": "Yashesh Shroff",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggu3bTBcysUxxtmnLI9n_OEGc92nfpZXGRAhDZ5vQ=s64",
      "userId": "12984101785953050215"
     },
     "user_tz": 420
    },
    "execution_millis": 415,
    "execution_start": 1603914507882,
    "id": "NStzHzfH5U17",
    "outputId": "48efa19f-9fe3-400a-81c5-8f1a91f05975",
    "output_cleared": false,
    "source_hash": "ae320f3a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['advance',\n",
       " 'beards',\n",
       " 'came',\n",
       " 'clothes',\n",
       " 'corn',\n",
       " 'creep',\n",
       " 'ears',\n",
       " 'fallen',\n",
       " 'field',\n",
       " 'flesh',\n",
       " 'forward',\n",
       " 'heard',\n",
       " 'impossible',\n",
       " 'interwoven',\n",
       " 'laid',\n",
       " 'pierced',\n",
       " 'pointed',\n",
       " 'rain',\n",
       " 'reapers',\n",
       " 'shift',\n",
       " 'stalks',\n",
       " 'step',\n",
       " 'strong',\n",
       " 'till',\n",
       " 'time',\n",
       " 'wind',\n",
       " 'yards']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a model of Bag of Words\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "docs = sent_tokenize(text)[703:706]\n",
    "docs\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer=CountVectorizer(stop_words='english')\n",
    "word_count_vector=count_vectorizer.fit_transform(docs)\n",
    "word_count_vector.shape\n",
    "word_count_vector.toarray()\n",
    "count_vectorizer.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01_NLP basics.ipynb",
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "ee38ac4f-e83b-4eca-957e-159aed17807b",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
